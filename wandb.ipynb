{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in ./pytorch_auto/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (2.30.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (1.22.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (58.1.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in ./pytorch_auto/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in ./pytorch_auto/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./pytorch_auto/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./pytorch_auto/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./pytorch_auto/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./pytorch_auto/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./pytorch_auto/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./pytorch_auto/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script needs these libraries to be installed:\n",
    "#   torch, torchvision, pytorch_lightning\n",
    "\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "from torch import optim, nn, utils\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3, inp_size=28, optimizer=\"Adam\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(inp_size * inp_size, 64), nn.ReLU(), nn.Linear(64, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, inp_size * inp_size)\n",
    "        )\n",
    "        self.lr = lr\n",
    "\n",
    "        # save hyperparameters to self.hparamsm auto-logged by wandb\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "\n",
    "        # log metrics to wandb\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "autoencoder = LitAutoEncoder(lr=1e-3, inp_size=28)\n",
    "\n",
    "# setup data\n",
    "batch_size = 32\n",
    "dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n",
    "train_loader = utils.data.DataLoader(dataset, shuffle=True)\n",
    "\n",
    "# initialise the wandb logger and name your wandb project\n",
    "wandb_logger = WandbLogger(project=\"my-awesome-project\")\n",
    "\n",
    "# add your batch size to the wandb config\n",
    "wandb_logger.experiment.config[\"batch_size\"] = batch_size\n",
    "\n",
    "# pass wandb_logger to the Trainer\n",
    "trainer = pl.Trainer(limit_train_batches=750, max_epochs=5, logger=wandb_logger)\n",
    "\n",
    "# train the model\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
